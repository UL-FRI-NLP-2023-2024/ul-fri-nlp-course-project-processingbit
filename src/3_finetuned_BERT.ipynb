{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_MODELS = {\n",
    "    'bert': 'bert-base-uncased',\n",
    "    'roberta': 'roberta-base',\n",
    "    'xlnet': 'xlnet-large-cased',\n",
    "    'xlm': 'xlm-mlm-en-2048',\n",
    "    'distilbert': 'distilbert-base-uncased',\n",
    "    'albert':'albert-base-v2'\n",
    "}\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'bert': (BertForSequenceClassification, BertTokenizer, BertConfig),\n",
    "}\n",
    "\n",
    "MODEL_TYPE = 'bert'\n",
    "PRETRAINED_MODEL_NAME = PRETRAINED_MODELS[MODEL_TYPE]\n",
    "\n",
    "model_class, tokenizer_class, config_class = MODEL_CLASSES[MODEL_TYPE]\n",
    "\n",
    "TRUNCATION = True\n",
    "LEARNING_RATE = 1e-5\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "LABELS = 'R2DiscussionType'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/dataset1.csv')\n",
    "# Dropping topic because it has only one value. It might be usefull if we had more data\n",
    "df = df.drop(columns=['Old Code Book', 'Memo', 'Chat0CREW1B', 'Chat0CREW1', 'Response Number', 'Message Time', 'CollapsR2DiscussionTypeInterpNothers', 'R2DiscussionTypeInterpNothers'])\n",
    "df.columns = [''.join([word[0].upper() + word[1:] for word in col.split()]) for col in df.columns]\n",
    "df.columns = [col.replace(' ', '') for col in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(609, 25)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Course', 'BookID', 'Topic', 'Bookclub', 'Pseudonym', 'Message',\n",
       "       'IsAnswer', 'Page', 'R1DiscussionType', 'R2DiscussionType',\n",
       "       'R1DialogicSpell', 'BinaryR1DialogicSpell', 'R1Uptake',\n",
       "       'BinaryR1Uptake', 'R2DialogicSpell', 'BinaryR2DialogicSpell',\n",
       "       'R2Uptake', 'BinaryR2Uptake', 'Pseudonym.1', 'Message.1', 'Bookclub.1',\n",
       "       'R1Question', 'R2Question', 'R1Pivot', 'R2Pivot'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If R1 has a value and R2 does not should we trust R1? Let us look at some of the examples to see if this is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column R2DialogicSpell has NaN where R1DialogicSpell has a value\n",
      "    R1DialogicSpell R2DialogicSpell\n",
      "240                             NaN\n",
      "261           Begin             NaN\n",
      "268                             NaN\n",
      "273                             NaN\n",
      "274                             NaN\n",
      "275                             NaN\n",
      "276                             NaN\n",
      "295           Break             NaN\n",
      "303           Begin             NaN\n",
      "315           Begin             NaN\n",
      "316                             NaN\n",
      "317                             NaN\n",
      "318                             NaN\n",
      "319                             NaN\n",
      "320                             NaN\n",
      "321                             NaN\n",
      "322                             NaN\n",
      "325                             NaN\n",
      "326             End             NaN\n",
      "362           Break             NaN\n",
      "396           Break             NaN\n",
      "501               2             NaN\n",
      "503               1             NaN\n",
      "507               1             NaN\n",
      "529               4             NaN\n",
      "576               8             NaN\n",
      "578               9             NaN\n",
      "579               9             NaN\n",
      "580               9             NaN\n",
      "581               9             NaN\n",
      "582               9             NaN\n",
      "583               9             NaN\n",
      "584               9             NaN\n",
      "588               8             NaN\n",
      "589               9             NaN\n",
      "590               9             NaN\n",
      "591               9             NaN\n",
      "592               9             NaN\n",
      "Column BinaryR2DialogicSpell has NaN where BinaryR1DialogicSpell has a value\n",
      "     BinaryR1DialogicSpell  BinaryR2DialogicSpell\n",
      "315                      1                    NaN\n",
      "316                      1                    NaN\n",
      "Column R2Uptake has NaN where R1Uptake has a value\n",
      "      R1Uptake R2Uptake\n",
      "321  Elaborate      NaN\n",
      "322     Affirm      NaN\n",
      "596     Prompt      NaN\n",
      "Filled R2Uptake with R1Uptake\n",
      "Column R2Question has NaN where R1Question has a value\n",
      "    R1Question R2Question\n",
      "272      C-LOT        NaN\n",
      "Filled R2Question with R1Question\n",
      "Column R2Pivot has NaN where R1Pivot has a value\n",
      "                                               R1Pivot R2Pivot\n",
      "208                Deliberation to Social/Procedure/UX     NaN\n",
      "209                Social/Procedure/UX to Deliberation     NaN\n",
      "226                            Seminar to Deliberation     NaN\n",
      "227                            Deliberation to Seminar     NaN\n",
      "233                Deliberation to Social/Procedure/UX     NaN\n",
      "234                Social/Procedure/UX to Deliberation     NaN\n",
      "235                Deliberation to Social/Procedure/UX     NaN\n",
      "236         Social/Procedure/UX to Social/Procedure/UX     NaN\n",
      "237         Social/Procedure/UX to Social/Procedure/UX     NaN\n",
      "238                Social/Procedure/UX to Deliberation     NaN\n",
      "245                            Deliberation to Seminar     NaN\n",
      "248                            Seminar to Deliberation     NaN\n",
      "249                            Deliberation to Seminar     NaN\n",
      "250                            Seminar to Deliberation     NaN\n",
      "254                            Deliberation to Seminar     NaN\n",
      "256                            Seminar to Deliberation     NaN\n",
      "261                            Deliberation to Seminar     NaN\n",
      "263                            Seminar to Deliberation     NaN\n",
      "269                            Deliberation to Seminar     NaN\n",
      "272                            Seminar to Deliberation     NaN\n",
      "273                            Deliberation to Seminar     NaN\n",
      "274                            Seminar to Deliberation     NaN\n",
      "284                Deliberation to Social/Procedure/UX     NaN\n",
      "285                Social/Procedure/UX to Deliberation     NaN\n",
      "286                Deliberation to Social/Procedure/UX     NaN\n",
      "294                     Seminar to Social/Procedure/UX     NaN\n",
      "295                     Seminar to Social/Procedure/UX     NaN\n",
      "296                 Social/Procedure/UX to Imaginative     NaN\n",
      "297    Imaginative to Deliberation/Social/Procedure/UX     NaN\n",
      "299                 Imaginative to Social/Procedure/UX     NaN\n",
      "300  Social/Procedure/UX to Deliberation to Social/...     NaN\n",
      "303         Social/Procedure/UX to Social/Procedure/UX     NaN\n",
      "322                 Seminar to Deliberation to Seminar     NaN\n",
      "323                        Seminar to Socal to Seminar     NaN\n",
      "324                            Seminar to Deliberation     NaN\n",
      "460          Seminar to Social/Procedure/UX to Seminar     NaN\n",
      "463          Seminar to Social/Procedure/UX to Seminar     NaN\n",
      "467                     Seminar to Social/Procedure/UX     NaN\n",
      "468                     Social/Procedure/UX to Seminar     NaN\n",
      "483                     Seminar to Social/Procedure/UX     NaN\n",
      "486          Seminar to Social/Procedure/UX to Seminar     NaN\n",
      "489          Seminar to Social/Procedure/UX to Seminar     NaN\n",
      "Filled R2Pivot with R1Pivot\n"
     ]
    }
   ],
   "source": [
    "# Identify the columns that start with R1 and R2 and have the same suffix\n",
    "r1_columns = [col for col in df.columns if col.startswith('R1') or col.startswith('BinaryR1')]\n",
    "r2_columns = [col for col in df.columns if col.startswith('R2') or col.startswith('BinaryR2')]\n",
    "\n",
    "# Make sure that the suffixes match\n",
    "matched_columns = [(r1, r2) for r1 in r1_columns for r2 in r2_columns if r1[2:] == r2[2:] or (r1[9:] == r2[9:] and r1.startswith('Binary') and r2.startswith('Binary'))]\n",
    "matched_columns\n",
    "#\n",
    "## Iterate over these pairs of columns\n",
    "for r1_col, r2_col in matched_columns:\n",
    "    # Find rows where the R2 version has NaN and the R1 version has a value\n",
    "    condition = (df[r2_col].isna() | df[r2_col].eq('')) & df[r1_col].notna()\n",
    "    if condition.any():# and \"Dialogic\" not in r1_col:\n",
    "        print(f'Column {r2_col} has NaN where {r1_col} has a value')\n",
    "        print(df.loc[condition, [r1_col, r2_col]])\n",
    "        if \"Pivot\" in r2_col or \"Upta\" in r2_col or \"Question\" in r2_col:\n",
    "            df.loc[condition, r2_col] = df.loc[condition, r1_col]\n",
    "            print(f'Filled {r2_col} with {r1_col}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to not be needed because all cases it is hard for me to tell which one is correct R1 or R2. Both answers seem valid so I will keep it as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=r1_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I remove duplicate columns and check if there are any values that are nan in the one I keep and not the other. In this case there were none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot1_columns = [col for col in df.columns if col.endswith('.1')]\n",
    "original_columns = [col.rstrip('.1') for col in dot1_columns if col.rstrip('.1') in df.columns]\n",
    "\n",
    "# For each pair of columns, if one column has a value and the other does not, keep the value\n",
    "for orig_col, dot1_col in zip(original_columns, dot1_columns):\n",
    "    if (df[orig_col].isna() & df[dot1_col].notna()).any():\n",
    "        print(f'Column {orig_col} has a value where {dot1_col} has NaN')\n",
    "\n",
    "# Remove the .1 columns\n",
    "df = df.drop(columns=dot1_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 discussion type columns. The collapsed one was apparently used for visualisation according to the given paper so it is not needed. This leaves us with R2DiscussionTypeInterpNothers and R2DiscussionType. The first seems to use interpreatation instead of seminar and that is not in the codebook so we will use the first column. I will now look at the unique values of the columns and see their counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.39080459770114945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_321966/412174125.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  print(counts[0]/counts.sum())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "R2Uptake\n",
       "NaN          238\n",
       "Affirm       141\n",
       "Elaborate    108\n",
       "Filler        61\n",
       "Clarify       50\n",
       "Disagree      10\n",
       "Prompt         1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = df['R2Uptake'].value_counts(dropna=False)\n",
    "print(counts[0]/counts.sum())\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can't train on one sample so I make it NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"R2Uptake\"] == \"Prompt\", \"R2Uptake\"] =  np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that there are not prompt or respond examples that can be found in the codebook. Also most are NaN which makes sense since maybe those are not uptakes. ALso NaNs are 40% of the data so we must achieve accuracy higher than 40 to have results better than just guessing NaN each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R2Question\n",
       "NaN              524\n",
       "C-LOT             37\n",
       "C-HOT             22\n",
       "O-HOT             11\n",
       "O-LOT              8\n",
       "C-LOT, C-LOT       3\n",
       "O-COT              1\n",
       "C-HOT, C-HOT       1\n",
       "O-HOT, C-LOT       1\n",
       "C-LOT? C-HOT?      1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['R2Question'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those who have repeating types can just be considered one type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['R2Question'] == \"C-LOT, C-LOT\", \"R2Question\"] = \"C-LOT\"\n",
    "df.loc[df['R2Question'] == \"C-HOT, C-HOT\", \"R2Question\"] = \"C-HOT\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O-COT does not exist in our Codebook so I looked at it and to me it looks like C-LOT since we have a probably a an answer in mind but we want clarification. This is also what R1 chose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "241    Which questions? like the prompt?\n",
       "Name: Message, dtype: object"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"R2Question\"] == 'O-COT'][\"Message\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['R2Question'] == 'O-COT', 'R2Question'] = 'C-LOT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360    Ok, so what happens then? Do we choose which d...\n",
       "Name: Message, dtype: object"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"R2Question\"] == 'O-HOT, C-LOT'][\"Message\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the above sentence we need to divide into two samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LABELS == \"R2Question\":\n",
    "    index = df.loc[df['Message'].str.startswith('Ok, so what happens then? Do we choose which door she opens?')].index[0]\n",
    "    new_row = df.iloc[index].copy()\n",
    "    new_row['Message'] =  \"Do we choose which door she opens?\"\n",
    "    new_row['R2Question'] = 'C-LOT'\n",
    "    new_row = pd.DataFrame(new_row).T\n",
    "    df.at[index, 'Message'] = \"Ok, so what happens then?\"\n",
    "    df.at[index, 'R2Question'] = 'O-HOT'\n",
    "    df = pd.concat([df.iloc[:index + 1, ], new_row, df.iloc[index + 1:, ]], ignore_index=True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the one with the '?' based on my and R1's judgment it is C-LOT. Since he askes a questions with an answer in mind.\n",
    "\n",
    "Whole question:\n",
    "\"I feel the flow doesn't entirely match the original story, but I felt it was awkwardly written and okay. We can add (ending) before it?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "407    I feel the flow doesn't entirely match the ori...\n",
       "Name: Message, dtype: object"
      ]
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"R2Question\"] == 'C-LOT? C-HOT?'][\"Message\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"R2Question\"] == 'C-LOT? C-HOT?', \"R2Question\"] = 'C-LOT'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have few samples compared to the total samples size that have a Question category. Also we have one sample with 2 labels. Which is likely not enough for our model to learn anything. This can probably be dropped when training and the ones that are C-LOT C-LOT can be converted just to a single C-LOT. Another options is to split the sentence somehow and classify separately and then combine the labels. This seems like a good idea and I might try it. For the C-LOT? C-HOT? it seems the expert is unsure what to write so we will either drop it or trust R1 which chose for that sample C-LOT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R2DiscussionType\n",
       "Seminar                  331\n",
       "Deliberation              85\n",
       "Social                    69\n",
       "UX                        47\n",
       "Procedure                 46\n",
       "Imaginative entry         17\n",
       "Other                      6\n",
       "Seminar, Deliberation      2\n",
       "Imaginative                2\n",
       "Social, Deliberation       1\n",
       "Deliberation, Seminar      1\n",
       "Social, Procedure          1\n",
       "Imaginative Entry          1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['R2DiscussionType'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R2Pivot\n",
       "NaN                                                           520\n",
       "Seminar to Deliberation                                        15\n",
       "Seminar to Social/Procedure/UX                                 14\n",
       "Deliberation to Seminar                                        10\n",
       "Deliberation to Social/Procedure/UX                             9\n",
       "Social/Procedure/UX to Seminar                                  9\n",
       "Social/Procedure/UX to Deliberation                             7\n",
       "Social/Procedure/UX to Social/Procedure/UX                      7\n",
       "Seminar to Social/Procedure/UX to Seminar                       4\n",
       "Imaginative entry to Seminar                                    2\n",
       "Seminar to Imaginative entry                                    2\n",
       "Imaginative to Deliberation/Social/Procedure/UX                 1\n",
       "Imaginative to Social/Procedure/UX                              1\n",
       "Social/Procedure/UX to Deliberation to Social/Procedure/UX      1\n",
       "Seminar to Deliberation to Seminar                              1\n",
       "Seminar to Socal to Seminar                                     1\n",
       "Seminar to Imaginative                                          1\n",
       "Imaginative to Seminar                                          1\n",
       "Delibration to Seminar                                          1\n",
       "Deliberationa to Seminar                                        1\n",
       "Social/Procedure/UX to Imaginative                              1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['R2Pivot'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"R2Pivot\"] == \"Deliberationa to Seminar\", \"R2Pivot\"] = \"Deliberation to Seminar\"\n",
    "df.loc[df[\"R2Pivot\"] == \"Delibration to Seminar\", \"R2Pivot\"] = \"Deliberation to Seminar\"\n",
    "df.loc[df[\"R2Pivot\"] == \"Imaginative to Seminar\", \"R2Pivot\"] = \"Imaginative entry to Seminar\"\n",
    "df.loc[df[\"R2Pivot\"] == \"Seminar to Imaginative\", \"R2Pivot\"] = \"Seminar to Imaginative entry\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89\n"
     ]
    }
   ],
   "source": [
    "value_counts = df['R2Pivot'].value_counts(dropna=True)\n",
    "total = value_counts.sum()\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this one we don't have many samples separately but maybe we can make a binary classifier and just take the previous and next category. The classifier will just say pivot yes or no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IsAnswer\n",
       "No     501\n",
       "NaN    106\n",
       "         2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['IsAnswer'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is Answer is either No or NaN. The NaN value does not mean it is an answer. I looked at the fields that were labelled NaN and they can also be a greeting which is not an answer or a statement which is not always an answer. Because of this I believe this column is not useful for training and I will drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['IsAnswer'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BookID\n",
       "260    421\n",
       "261    188\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['BookID'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bookclub\n",
       "Book Club One      275\n",
       "Book Club Four     192\n",
       "Book Club Two       94\n",
       "Book Club Three     30\n",
       "Book Club Five      18\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Bookclub'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For BookID, Page and Bookclub I am unsure if they will be usefull in the end. My reasoning is because we want to generalise and what if we add a Bookclub 6 and if the model has never seen it. Or if we add a new book but the model has learned something about the specific books 260 and 261. Because of this I believe it is better to drop these columns. I was considering maybe they could be used for ids of different discussions since only a club or only a book is not enough to determine a discussion and we want to classify sentences in separate probably. If we want context we can try to use history instead of these ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PrevMessage'] = ''\n",
    "for i in range(1, len(df)):\n",
    "    if (df.loc[i, 'BookID'] == df.loc[i-1, 'BookID'] and\n",
    "        df.loc[i, 'Bookclub'] == df.loc[i-1, 'Bookclub'] and\n",
    "        df.loc[i, 'Course'] == df.loc[i-1, 'Course']):\n",
    "        \n",
    "        df.loc[i, 'PrevMessage'] = df.loc[i-1, 'Message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_321966/3560292819.py:7: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  dfcp = dfcp.fillna('')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max_length: test\n"
     ]
    }
   ],
   "source": [
    "prev_3 = []\n",
    "\n",
    "prev_categories = [''] * len(df)\n",
    "\n",
    "dfcp = df.copy()\n",
    "dfcp['Page'] = dfcp['Page'].astype(str)\n",
    "dfcp = dfcp.fillna('')\n",
    "\n",
    "for i in range(1, len(df)):\n",
    "    if dfcp.iloc[i][['BookID', 'Bookclub', 'Course']].equals(dfcp.iloc[i-1][['BookID', 'Bookclub', 'Course']]):\n",
    "        if pd.isna(dfcp.iloc[i-1][LABELS]):\n",
    "            prev_3.append('')\n",
    "            continue\n",
    "\n",
    "        prev_3.append(dfcp.iloc[i-1]['R2DiscussionType'] + '-' \\\n",
    "                        +  dfcp.iloc[i-1]['R2Uptake'] + '-' \\\n",
    "                        + dfcp.iloc[i-1]['Pseudonym'])\n",
    "        \n",
    "        if len(prev_3) > 3:\n",
    "            prev_3.pop(0)\n",
    "    else:\n",
    "        prev_3 = []\n",
    "    prev_categories[i] = '|'.join(prev_3)\n",
    "\n",
    "df['PrevCategories'] = prev_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So, where have we landed: Lady or tiger? Trial or coverup? I think we just have to write something in the space below.\n",
      "Seminar to Deliberation\n",
      "Seminar Seminar, Deliberation\n",
      "I agree that the princess could just banish the new couple away too. Do we have to agree on a single ending?\n",
      "Seminar to Deliberation\n",
      "Seminar Seminar, Deliberation\n",
      "I don't think we have to agree on a single ending as both are viable. I still will go with the Lady as being behind the curtain. The King will be upset with this but will forgive his daughter. He may do something else to the boyfriend later.\n",
      "Deliberation to Seminar\n",
      "Seminar, Deliberation Seminar\n",
      "Ok, I had to reset the book to make this work!\n",
      "Social/Procedure/UX to Social/Procedure/UX\n",
      "Deliberation Deliberation\n",
      "Yay you got it!\n",
      "Social/Procedure/UX to Social/Procedure/UX\n",
      "Deliberation Social, Deliberation\n",
      "Live without was the first one I put. Then Trust, Love, and then Know\n",
      "Social/Procedure/UX to Deliberation\n",
      "Social, Deliberation Deliberation\n",
      "Oh I see, there are two questions here. I feel that the princess would have led him to the tiger\n",
      "Deliberation to Seminar\n",
      "Deliberation Deliberation, Seminar\n",
      "As for the second question I believe that the king would try to cover up the fact she broke the law. I only say that because if he was that mad because someone was her lover I doubt he would send her to die. He would probably rather punish her in private.\n",
      "Deliberation to Seminar\n",
      "UX Seminar\n",
      "im just back at the screen we were at before\n",
      "Seminar to Deliberation\n",
      "Deliberation UX\n",
      "Curiosity got the best of me, and I googled this book, which I'd never heard of despite it being a well-known allegory. It presents an unsolvable problem, yet we all felt pretty confident taking a stab at answering the prompts. I wonder what we are to make of all of us feeling pretty certain the princess chose the tiger. Is our choice based on the author's characterization of her or our belief that human nature is fundamentally selfish?\n",
      "Social/Procedure/UX to Imaginative\n",
      "Procedure Seminar\n",
      "I don't think I should have gone outside this platform to Google the book since this is supposed to be a discussion between us via this platform. Sorry about that.\n",
      "Imaginative to Deliberation/Social/Procedure/UX\n",
      "Seminar Deliberation\n",
      "Cheryl Diaz, you bring up a good point - are we truly using the information gained through the reading or our own beliefs that human nature is fundamentally selfish. You really have me thinking about this.\n",
      "Seminar to Social/Procedure/UX\n",
      "Deliberation Seminar\n",
      "Since we have each answered the two questions that were proposed, maybe choosing a day/time to all discuss our final answer to the prompt would be best. I am pretty flexible and am available today-thursday just about all day.\n",
      "Imaginative to Social/Procedure/UX\n",
      "Seminar Procedure\n",
      "I think she sends him to the door with the tiger. When the king finds out that she knew he is so pleased by her barbarism that he does nothing but inside he has now decided that she is a worthy predecessor.\n",
      "Deliberation to Seminar\n",
      "Social Seminar\n",
      "0.8426966292134831\n"
     ]
    }
   ],
   "source": [
    "# Select all rows where 'R2Pivot' is not NaN\n",
    "non_nan_rows = df[df['R2Pivot'].notna()]\n",
    "\n",
    "# Get the indices of the non-NaN rows\n",
    "indices = non_nan_rows.index\n",
    "\n",
    "# Print the current 'R2DiscussionType' for these rows and the 'R2DiscussionType' from the previous row in the original DataFrame\n",
    "res = zip(non_nan_rows['R2DiscussionType'], df.loc[indices - 1, 'R2DiscussionType'] if indices[0] > 0 else None)\n",
    "\n",
    "counter = 0\n",
    "for idx, (next, prev) in enumerate(res):\n",
    "    if prev in non_nan_rows.iloc[idx]['R2Pivot'] and next in non_nan_rows.iloc[idx]['R2Pivot']:\n",
    "        counter += 1\n",
    "    else:\n",
    "        print(non_nan_rows.iloc[idx]['Message'])\n",
    "        print(non_nan_rows.iloc[idx]['R2Pivot'])\n",
    "        print(prev, next)\n",
    "\n",
    "print(counter/len(non_nan_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R2DiscussionType\n",
       "Seminar                  331\n",
       "Deliberation              85\n",
       "Social                    69\n",
       "UX                        47\n",
       "Procedure                 46\n",
       "Imaginative entry         17\n",
       "Other                      6\n",
       "Seminar, Deliberation      2\n",
       "Imaginative                2\n",
       "Social, Deliberation       1\n",
       "Deliberation, Seminar      1\n",
       "Social, Procedure          1\n",
       "Imaginative Entry          1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['R2DiscussionType'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LABELS == \"R2DiscussionType\":\n",
    "    df.loc[df['R2DiscussionType'] == 'Imaginative Entry', 'R2DiscussionType'] = 'Imaginative entry'\n",
    "    df.loc[df['R2DiscussionType'] == 'Imaginative', 'R2DiscussionType'] = 'Imaginative entry'\n",
    "    df.loc[df['R2DiscussionType'] == 'Deliberation, Seminar', 'R2DiscussionType'] = 'Seminar, Deliberation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237    Yay you got it!\n",
       "Name: Message, dtype: object"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['R2DiscussionType'] == 'Social, Deliberation']['Message']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example seems like only Social to me. It can also be procedure due to you got it. Since it is only one sample and I do not believe it will effect our results I will just mark it as social."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LABELS == \"R2DiscussionType\":\n",
    "    df.loc[df['R2DiscussionType'] == 'Social, Deliberation', 'R2DiscussionType'] = 'Social'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202    So, where have we landed: Lady or tiger? Trial...\n",
       "226    I agree that the princess could just banish th...\n",
       "245    Oh I see, there are two questions here. I feel...\n",
       "Name: Message, dtype: object"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['R2DiscussionType'] == 'Seminar, Deliberation']['Message']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full versions: \\\n",
    "1: So, where have we landed: Lady or tiger? Trial or coverup? I think we just have to write something in the space below. \\\n",
    "2: I agree that the princess could just banish the new couple away too. Do we have to agree on a single ending? \\\n",
    "3: Oh I see, there are two questions here. I feel that the princess would have led him to the tiger \n",
    "\n",
    "These are combination of two sentences in a single sample that each are from a different category. I will divide the sample into two and mark each with the corresponding category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LABELS == \"R2DiscussionType\":\n",
    "    index = df.loc[df['Message'].str.startswith('Oh I see, there are two questions here')].index[0]\n",
    "    new_row = df.iloc[index].copy()\n",
    "    new_row['Message'] = \"I feel that the princess would have led him to the tiger\"\n",
    "    new_row['R2DiscussionType'] = 'Seminar'\n",
    "    new_row = pd.DataFrame(new_row).T\n",
    "    df.at[index, 'Message'] = \"Oh I see, there are two questions here.\"\n",
    "    df.at[index, 'R2DiscussionType'] = 'Deliberation'\n",
    "    df = pd.concat([df.iloc[:index + 1, ], new_row, df.iloc[index + 1:, ]], ignore_index=True) \n",
    "\n",
    "    index = df.loc[df['Message'].str.startswith('I agree that the princess could just banish the new couple away too.')].index[0]\n",
    "    new_row = df.iloc[index].copy()\n",
    "    new_row['Message'] = \"Do we have to agree on a single ending?\"\n",
    "    new_row['R2DiscussionType'] = 'Deliberation'\n",
    "    new_row = pd.DataFrame(new_row).T\n",
    "    df.at[index, 'Message'] = \"I agree that the princess could just banish the new couple away too.\"\n",
    "    df.at[index, 'R2DiscussionType'] = 'Seminar'\n",
    "    df = pd.concat([df.iloc[:index + 1, ], new_row, df.iloc[index + 1:, ]], ignore_index=True) \n",
    "\n",
    "    index = df.loc[df['Message'].str.startswith('So, where have we landed: Lady or tiger? Trial or coverup?')].index[0]\n",
    "    new_row = df.iloc[index].copy()\n",
    "    new_row['Message'] = \"I think we just have to write something in the space below.\"\n",
    "    new_row['R2DiscussionType'] = 'Deliberation'\n",
    "    new_row = pd.DataFrame(new_row).T\n",
    "    df.at[index, 'Message'] = \"So, where have we landed: Lady or tiger? Trial or coverup?\"\n",
    "    df.at[index, 'R2DiscussionType'] = 'Seminar'\n",
    "    df = pd.concat([df.iloc[:index + 1, ], new_row, df.iloc[index + 1:, ]], ignore_index=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "315    Looks good! I guess we can complete the post s...\n",
       "Name: Message, dtype: object"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['R2DiscussionType'] == 'Social, Procedure']['Message']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here Looks good is the social part and the rest the Procedure so I will divide again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LABELS == \"R2DiscussionType\":\n",
    "    index = df.loc[df['Message'].str.startswith('Looks good! I guess we can complete the post survey')].index[0]\n",
    "    new_row = df.iloc[index].copy()\n",
    "    new_row['Message'] = \"I guess we can complete the post survey and submit our assignment\"\n",
    "    new_row['R2DiscussionType'] = 'Procedure'\n",
    "    new_row = pd.DataFrame(new_row).T\n",
    "    df.at[index, 'Message'] = \"Looks good!\"\n",
    "    df.at[index, 'R2DiscussionType'] = 'Social'\n",
    "    df = pd.concat([df.iloc[:index + 1, ], new_row, df.iloc[index + 1:, ]], ignore_index=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R2DiscussionType\n",
       "Seminar              334\n",
       "Deliberation          88\n",
       "Social                71\n",
       "Procedure             47\n",
       "UX                    47\n",
       "Imaginative entry     20\n",
       "Other                  6\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.value_counts('R2DiscussionType')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "class R2UptakeDataset(Dataset):\n",
    "    def __init__(self, X, y, tokenizer, max_length, enc):\n",
    "        self.X = X.values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        # Fit the label binarizer and transform the labels into one-hot encoded format\n",
    "        self.labels = enc.transform(y.values.reshape(-1, 1)).toarray()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Encode the utterance using the provided tokenizer\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            self.X[idx],\n",
    "            add_special_tokens=True,\n",
    "            max_length = self.max_length,\n",
    "            return_token_type_ids=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            truncation=TRUNCATION,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        # Convert the list of strings into a one-hot encoded format\n",
    "        label = self.labels[idx]  # This should now be a binary vector instead of a list of strings\n",
    "\n",
    "        # Return the encoding and the label\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.float),\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_321966/75132861.py:2: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  X = df.drop(columns=r2columns).fillna('')\n"
     ]
    }
   ],
   "source": [
    "r2columns = [col for col in df.columns if 'R2' in col]\n",
    "X = df.drop(columns=r2columns).fillna('')\n",
    "y = df[LABELS].fillna('None')\n",
    "\n",
    "X[\"texts\"] = X['Message'] + \"|\" + X['PrevCategories'] + \"|\" + X['Pseudonym']\n",
    "\n",
    "if LABELS == 'R2Uptake':\n",
    "    X[\"texts\"] = X[\"texts\"] + \"-\" + X['R2DiscussionType']\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R2DiscussionType\n",
       "Seminar              334\n",
       "Deliberation          88\n",
       "Social                71\n",
       "Procedure             47\n",
       "UX                    47\n",
       "Imaginative entry     20\n",
       "Other                  6\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tmp, X_test, y_train_tmp, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_tmp, y_train_tmp, test_size=0.3, random_state=42, stratify=y_train_tmp)\n",
    "\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(y_train.values.reshape(-1, 1))\n",
    "\n",
    "num_labels = len(y_train.unique())\n",
    "num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2DiscussionType\n",
      "Seminar              187\n",
      "Deliberation          49\n",
      "Social                40\n",
      "UX                    27\n",
      "Procedure             26\n",
      "Imaginative entry     11\n",
      "Other                  3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_texts = X_train['texts']\n",
    "train_labels = y_train\n",
    "\n",
    "val_texts = X_val['texts']\n",
    "val_labels = y_val\n",
    "\n",
    "test_texts = X_test['texts']\n",
    "test_labels = y_test\n",
    "\n",
    "longest_train_data = train_texts[train_texts.str.len().idxmax()]\n",
    "max_length = min(2 ** (len(tokenizer.tokenize(longest_train_data))-1).bit_length(), 512)\n",
    "print(train_labels.value_counts())\n",
    "\n",
    "train_data = R2UptakeDataset(train_texts, train_labels, tokenizer,max_length, enc)\n",
    "val_data = R2UptakeDataset(val_texts, val_labels, tokenizer, max_length, enc)\n",
    "test_data = R2UptakeDataset(test_texts, test_labels, tokenizer, max_length, enc)\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_labels_encoded = le.fit_transform(train_labels)\n",
    "val_labels_encoded = le.transform(val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R2DiscussionType\n",
       "Seminar              187\n",
       "Deliberation          49\n",
       "Social                40\n",
       "UX                    27\n",
       "Procedure             26\n",
       "Imaginative entry     11\n",
       "Other                  3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I think the tiger was behind the door. Mostly because of the princess being described as \"hot-blooded and semi-barbaric.\"|Seminar|Seminar|Seminar'"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Deliberation'"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#HIDDEN_WEIGHTS = 768\n",
    "HIDDEN_WEIGHTS = 256\n",
    "DROPOUT = 0.3\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self, pretrained_model_name, num_labels):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.l1 = model_class.from_pretrained(pretrained_model_name, num_labels=self.num_labels)\n",
    "        self.pre_classifier = torch.nn.Linear(self.num_labels, HIDDEN_WEIGHTS)\n",
    "        self.dropout = torch.nn.Dropout(DROPOUT)\n",
    "        self.classifier = torch.nn.Linear(HIDDEN_WEIGHTS, self.num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        output = output.view(-1, self.num_labels)  # Reshape the output\n",
    "        return output\n",
    "\n",
    "model = BERTClass(PRETRAINED_MODEL_NAME, num_labels)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "\n",
    "weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    weights = weights.to('cuda')\n",
    "    \n",
    "criterion = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    return criterion(outputs, targets)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, valid_dataloader):\n",
    "    val_targets = []\n",
    "    val_outputs = []\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_dataloader):\n",
    "            input_ids = batch['input_ids'].to(device, dtype=torch.long)\n",
    "            attention_mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
    "            token_type_ids = batch['token_type_ids'].to(device, dtype=torch.long)\n",
    "            labels = batch['labels'].to(device, dtype=torch.float)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            val_targets.extend(labels.cpu().detach().numpy().tolist())\n",
    "            val_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "            \n",
    "\n",
    "    val_loss /= len(valid_dataloader)\n",
    "    \n",
    "    return val_loss, val_targets, val_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ckp(checkpoint_fpath, model, enc):\n",
    "    checkpoint = torch.load(checkpoint_fpath, map_location=device)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    enc.set_params(**checkpoint['enc'])\n",
    "    return model, enc\n",
    "\n",
    "def save_ckp(state, best_model_path):\n",
    "    torch.save(state, best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        input_ids = batch['input_ids'].to(device, dtype=torch.long)\n",
    "        attention_mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
    "        token_type_ids = batch['token_type_ids'].to(device, dtype=torch.long)\n",
    "        labels = batch['labels'].to(device, dtype=torch.float)\n",
    "\n",
    "        model.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    \n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_epochs, train_dataloader, valid_dataloader, model, optimizer, best_model_path = \"./\", patience=1):\n",
    "    valid_loss_min = np.Inf\n",
    "\n",
    "    num_not_improved = 0\n",
    "    for epoch in range(1, num_epochs):\n",
    "        print()\n",
    "        print(\"#################### Epoch {}: Training Start    ####################\".format(epoch))\n",
    "        train_loss = train(model, train_dataloader)\n",
    "        print('#################### Epoch {}: Training End      ####################'.format(epoch))\n",
    "\n",
    "        print()\n",
    "        print(\"#################### Epoch {}: Validation Start ####################\".format(epoch))\n",
    "\n",
    "        valid_loss, val_targets, val_outputs = valid(model, valid_dataloader)\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, train_loss, valid_loss))\n",
    "\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min, valid_loss))\n",
    "\n",
    "            checkpoint = {\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'enc' : enc.get_params()\n",
    "                    }\n",
    "\n",
    "            save_ckp(checkpoint, best_model_path)\n",
    "            valid_loss_min = valid_loss\n",
    "            num_not_improved = 0\n",
    "        else:\n",
    "            num_not_improved += 1\n",
    "            if num_not_improved >= patience:\n",
    "                print('Not improvement for more than:', num_not_improved)\n",
    "                break\n",
    "            \n",
    "        print(\"#################### Epoch {}: Validation End   ####################\".format(epoch))\n",
    "        print()\n",
    "\n",
    "    print(\"#################### Training finished     ####################\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('No saved model found. Need to be train from scratch.')\n",
    "trained_model = train_model(EPOCHS, train_loader, val_loader, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:45<00:00, 11.41s/it]\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_labels , test_predictions_probs = valid(trained_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4385611116886139"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = [ prob_list == np.max(prob_list) for prob_list in test_predictions_probs ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy:', accuracy_score(test_labels, test_predictions))\n",
    "print('Precision:', precision_score(test_labels, test_predictions, average='weighted'))\n",
    "print('Recall:', recall_score(test_labels, test_predictions, average='weighted'))\n",
    "print('F1:', f1_score(test_labels, test_predictions, average='weighted'))\n",
    "\n",
    "report = classification_report(test_labels, test_predictions, target_names=enc.categories_[0])\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IDEAS: Try to give the models also the users when passing previous categories. In the case of uptake it might be usefull? Cause we will be uptaking from him if it is only us speaking it is less likely to have an uptake."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
