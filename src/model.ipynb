{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikolay/EMAI/Ljublajna/NLP/ul-fri-nlp-course-project-processingbit/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import AdamW\n",
    "import torch\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAINED_MODELS = {\n",
    "    'bert': 'bert-base-uncased',\n",
    "    'roberta': 'roberta-base',\n",
    "    'xlnet': 'xlnet-large-cased',\n",
    "    'xlm': 'xlm-mlm-en-2048',\n",
    "    'distilbert': 'distilbert-base-uncased',\n",
    "    'albert':'albert-base-v2'\n",
    "}\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'bert': (BertForSequenceClassification, BertTokenizer, BertConfig),\n",
    "    #'roberta': (RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig),\n",
    "    #'xlnet': (XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig),\n",
    "    #'xlm': (XLMForSequenceClassification, XLMTokenizer, XLMConfig),\n",
    "    #'distilbert': (DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig),\n",
    "    #'albert':(AlbertForSequenceClassification,AlbertTokenizer, AlbertConfig)\n",
    "}\n",
    "\n",
    "MODEL_TYPE = 'bert'\n",
    "PRETRAINED_MODEL_NAME = PRETRAINED_MODELS[MODEL_TYPE]\n",
    "\n",
    "model_class, tokenizer_class, config_class = MODEL_CLASSES[MODEL_TYPE]\n",
    "\n",
    "TRUNCATION = True\n",
    "LEARNING_RATE = 1e-5\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "LABELS = 'R2DiscussionType'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/dataset1.csv')\n",
    "# Dropping topic because it has only one value. It might be usefull if we had more data\n",
    "df = df.drop(columns=['Old Code Book', 'Memo', 'Chat0CREW1B', 'Chat0CREW1', 'Response Number', 'Message Time', 'CollapsR2DiscussionTypeInterpNothers', 'R2DiscussionTypeInterpNothers'])\n",
    "df.columns = [''.join([word[0].upper() + word[1:] for word in col.split()]) for col in df.columns]\n",
    "df.columns = [col.replace(' ', '') for col in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(609, 25)\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Course', 'BookID', 'Topic', 'Bookclub', 'Pseudonym', 'Message',\n",
       "       'IsAnswer', 'Page', 'R1DiscussionType', 'R2DiscussionType',\n",
       "       'R1DialogicSpell', 'BinaryR1DialogicSpell', 'R1Uptake',\n",
       "       'BinaryR1Uptake', 'R2DialogicSpell', 'BinaryR2DialogicSpell',\n",
       "       'R2Uptake', 'BinaryR2Uptake', 'Pseudonym.1', 'Message.1', 'Bookclub.1',\n",
       "       'R1Question', 'R2Question', 'R1Pivot', 'R2Pivot'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If R1 has a value and R2 does not should we trust R1? Let us look at some of the examples to see if this is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column R2DialogicSpell has NaN where R1DialogicSpell has a value\n",
      "    R1DialogicSpell R2DialogicSpell  BinaryR2Uptake\n",
      "240                             NaN               0\n",
      "261           Begin             NaN               0\n",
      "268                             NaN               0\n",
      "273                             NaN               1\n",
      "274                             NaN               0\n",
      "                                               Message\n",
      "240  she was smiling by the end and the book told m...\n",
      "261  As for the second question I believe that the ...\n",
      "268               Hey guys I got through the next page\n",
      "273                            I know live without him\n",
      "274       im just back at the screen we were at before\n",
      "Column BinaryR2DialogicSpell has NaN where BinaryR1DialogicSpell has a value\n",
      "     BinaryR1DialogicSpell  BinaryR2DialogicSpell  BinaryR2Uptake\n",
      "315                      1                    NaN               0\n",
      "316                      1                    NaN               0\n",
      "                                               Message\n",
      "315  I think she sends him to the door with the tig...\n",
      "316                             Ha, I meant successor.\n",
      "Column R2Uptake has NaN where R1Uptake has a value\n",
      "      R1Uptake R2Uptake  BinaryR2Uptake\n",
      "321  Elaborate      NaN               0\n",
      "322     Affirm      NaN               0\n",
      "596     Prompt      NaN               0\n",
      "                                               Message\n",
      "321  But I think he would also revel in the fact th...\n",
      "322  Just realized I never answered the second ques...\n",
      "596  I certainly hoped that she was steering her lo...\n",
      "Column R2Question has NaN where R1Question has a value\n",
      "    R1Question R2Question  BinaryR2Uptake\n",
      "272      C-LOT        NaN               1\n",
      "                                 Message\n",
      "272  Did you guys get the final question\n",
      "Column R2Pivot has NaN where R1Pivot has a value\n",
      "                                 R1Pivot R2Pivot  BinaryR2Uptake\n",
      "208  Deliberation to Social/Procedure/UX     NaN               1\n",
      "209  Social/Procedure/UX to Deliberation     NaN               1\n",
      "226              Seminar to Deliberation     NaN               1\n",
      "227              Deliberation to Seminar     NaN               1\n",
      "233  Deliberation to Social/Procedure/UX     NaN               1\n",
      "                                               Message\n",
      "208                                             Great!\n",
      "209  Looks good. I think to mark our assignment don...\n",
      "226  I agree that the princess could just banish th...\n",
      "227  I don't think we have to agree on a single end...\n",
      "233  So the top is for chatting and the bottom is f...\n"
     ]
    }
   ],
   "source": [
    "# Identify the columns that start with R1 and R2 and have the same suffix\n",
    "r1_columns = [col for col in df.columns if col.startswith('R1') or col.startswith('BinaryR1')]\n",
    "r2_columns = [col for col in df.columns if col.startswith('R2') or col.startswith('BinaryR2')]\n",
    "\n",
    "# Make sure that the suffixes match\n",
    "matched_columns = [(r1, r2) for r1 in r1_columns for r2 in r2_columns if r1[2:] == r2[2:] or (r1[9:] == r2[9:] and r1.startswith('Binary') and r2.startswith('Binary'))]\n",
    "matched_columns\n",
    "#\n",
    "## Iterate over these pairs of columns\n",
    "for r1_col, r2_col in matched_columns:\n",
    "    # Find rows where the R2 version has NaN and the R1 version has a value\n",
    "    condition = df[r2_col].isna() & df[r1_col].notna()\n",
    "    if condition.any():# and \"Dialogic\" not in r1_col:\n",
    "        print(f'Column {r2_col} has NaN where {r1_col} has a value')\n",
    "        print(df.loc[condition, [r1_col, r2_col, 'BinaryR2Uptake']].head())\n",
    "        print(df.loc[condition, ['Message']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to not be needed because all cases it is hard for me to tell which one is correct R1 or R2. Both answers seem valid so I will keep it as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=r1_columns, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I remove duplicate columns and check if there are any values that are nan in the one I keep and not the other. In this case there were none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot1_columns = [col for col in df.columns if col.endswith('.1')]\n",
    "original_columns = [col.rstrip('.1') for col in dot1_columns if col.rstrip('.1') in df.columns]\n",
    "\n",
    "# For each pair of columns, if one column has a value and the other does not, keep the value\n",
    "for orig_col, dot1_col in zip(original_columns, dot1_columns):\n",
    "    if (df[orig_col].isna() & df[dot1_col].notna()).any():\n",
    "        print(f'Column {orig_col} has a value where {dot1_col} has NaN')\n",
    "\n",
    "# Remove the .1 columns\n",
    "df = df.drop(columns=dot1_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 discussion type columns. The collapsed one was apparently used for visualisation according to the given paper so it is not needed. This leaves us with R2DiscussionTypeInterpNothers and R2DiscussionType. The first seems to use interpreatation instead of seminar and that is not in the codebook so we will use the first column. I will now look at the unique values of the columns and see their counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3957307060755337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_306344/412174125.py:2: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  print(counts[0]/counts.sum())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "R2Uptake\n",
       "NaN          241\n",
       "Affirm       140\n",
       "Elaborate    107\n",
       "Filler        61\n",
       "Clarify       50\n",
       "Disagree      10\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = df['R2Uptake'].value_counts(dropna=False)\n",
    "print(counts[0]/counts.sum())\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that there are not prompt or respond examples that can be found in the codebook. Also most are NaN which makes sense since maybe those are not uptakes. ALso NaNs are 40% of the data so we must achieve accuracy higher than 40 to have results better than just guessing NaN each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R2Question\n",
       "NaN      525\n",
       "C-LOT     42\n",
       "C-HOT     23\n",
       "O-HOT     12\n",
       "O-LOT      8\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['R2Question'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those who have repeating types can just be considered one type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['R2Question'] == \"C-LOT, C-LOT\", \"R2Question\"] = \"C-LOT\"\n",
    "df.loc[df['R2Question'] == \"C-HOT, C-HOT\", \"R2Question\"] = \"C-HOT\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O-COT does not exist in our Codebook so I looked at it and to me it looks like C-LOT since we have a probably a an answer in mind but we want clarification. This is also what R1 chose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "241    Which questions? like the prompt?\n",
       "Name: Message, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"R2Question\"] == 'O-COT'][\"Message\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['R2Question'] == 'O-COT', 'R2Question'] = 'C-LOT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360    Ok, so what happens then? Do we choose which d...\n",
       "Name: Message, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"R2Question\"] == 'O-HOT, C-LOT'][\"Message\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the above sentence we need to divide into two samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = 'R2Question'\n",
    "if LABELS == \"R2Question\":\n",
    "    index = df.loc[df['Message'].str.startswith('Ok, so what happens then? Do we choose which door she opens?')].index[0]\n",
    "    new_row = df.iloc[index].copy()\n",
    "    new_row['Message'] =  \"Do we choose which door she opens?\"\n",
    "    new_row['R2Question'] = 'C-LOT'\n",
    "    new_row = pd.DataFrame(new_row).T\n",
    "    df.at[index, 'Message'] = \"Ok, so what happens then?\"\n",
    "    df.at[index, 'R2Question'] = 'O-HOT'\n",
    "    df = pd.concat([df.iloc[:index + 1, ], new_row, df.iloc[index + 1:, ]], ignore_index=True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the one with the '?' based on my and R1's judgment it is C-LOT. Since he askes a questions with an answer in mind.\n",
    "\n",
    "Whole question:\n",
    "\"I feel the flow doesn't entirely match the original story, but I felt it was awkwardly written and okay. We can add (ending) before it?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "407    I feel the flow doesn't entirely match the ori...\n",
       "Name: Message, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"R2Question\"] == 'C-LOT? C-HOT?'][\"Message\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"R2Question\"] == 'C-LOT? C-HOT?', \"R2Question\"] = 'C-LOT'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have few samples compared to the total samples size that have a Question category. Also we have one sample with 2 labels. Which is likely not enough for our model to learn anything. This can probably be dropped when training and the ones that are C-LOT C-LOT can be converted just to a single C-LOT. Another options is to split the sentence somehow and classify separately and then combine the labels. This seems like a good idea and I might try it. For the C-LOT? C-HOT? it seems the expert is unsure what to write so we will either drop it or trust R1 which chose for that sample C-LOT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R2DiscussionType\n",
       "Seminar                  331\n",
       "Deliberation              85\n",
       "Social                    69\n",
       "UX                        47\n",
       "Procedure                 46\n",
       "Imaginative entry         17\n",
       "Other                      6\n",
       "Seminar, Deliberation      2\n",
       "Imaginative                2\n",
       "Social, Deliberation       1\n",
       "Deliberation, Seminar      1\n",
       "Social, Procedure          1\n",
       "Imaginative Entry          1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['R2DiscussionType'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R2Pivot\n",
       "NaN                                           562\n",
       "Seminar to Social/Procedure/UX                 10\n",
       "Social/Procedure/UX to Seminar                  8\n",
       "Seminar to Deliberation                         7\n",
       "Deliberation to Social/Procedure/UX             4\n",
       "Social/Procedure/UX to Social/Procedure/UX      4\n",
       "Social/Procedure/UX to Deliberation             3\n",
       "Deliberation to Seminar                         3\n",
       "Seminar to Imaginative entry                    2\n",
       "Imaginative entry to Seminar                    2\n",
       "Seminar to Imaginative                          1\n",
       "Imaginative to Seminar                          1\n",
       "Delibration to Seminar                          1\n",
       "Deliberationa to Seminar                        1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['R2Pivot'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    }
   ],
   "source": [
    "value_counts = df['R2Pivot'].value_counts(dropna=True)\n",
    "total = value_counts.sum()\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this one we don't have many samples separately but maybe we can make a binary classifier and just take the previous and next category. The classifier will just say pivot yes or no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IsAnswer\n",
       "No     501\n",
       "NaN    106\n",
       "         2\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['IsAnswer'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is Answer is either No or NaN. The NaN value does not mean it is an answer. I looked at the fields that were labelled NaN and they can also be a greeting which is not an answer or a statement which is not always an answer. Because of this I believe this column is not useful for training and I will drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['IsAnswer'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BookID\n",
       "260    421\n",
       "261    188\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['BookID'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bookclub\n",
       "Book Club One      275\n",
       "Book Club Four     192\n",
       "Book Club Two       94\n",
       "Book Club Three     30\n",
       "Book Club Five      18\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Bookclub'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For BookID, Page and Bookclub I am unsure if they will be usefull in the end. My reasoning is because we want to generalise and what if we add a Bookclub 6 and if the model has never seen it. Or if we add a new book but the model has learned something about the specific books 260 and 261. Because of this I believe it is better to drop these columns. I was considering maybe they could be used for ids of different discussions since only a club or only a book is not enough to determine a discussion and we want to classify sentences in separate probably. If we want context we can try to use history instead of these ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PrevMessage'] = ''\n",
    "for i in range(1, len(df)):\n",
    "    if (df.loc[i, 'BookID'] == df.loc[i-1, 'BookID'] and\n",
    "        df.loc[i, 'Bookclub'] == df.loc[i-1, 'Bookclub'] and\n",
    "        df.loc[i, 'Course'] == df.loc[i-1, 'Course']):\n",
    "        \n",
    "        df.loc[i, 'PrevMessage'] = df.loc[i-1, 'Message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_3 = []\n",
    "\n",
    "prev_categories = [''] * len(df)\n",
    "\n",
    "for i in range(1, len(df)):\n",
    "    if df.iloc[i][['BookID', 'Bookclub', 'Course']].equals(df.iloc[i-1][['BookID', 'Bookclub', 'Course']]):\n",
    "        if pd.isna(df.iloc[i-1][LABELS]):\n",
    "            prev_3.append('')\n",
    "            continue\n",
    "\n",
    "        prev_3.append(df.iloc[i-1][LABELS])\n",
    "        if len(prev_3) > 3:\n",
    "            prev_3.pop(0)\n",
    "    else:\n",
    "        prev_3 = []\n",
    "    prev_categories[i] = '|'.join(prev_3)\n",
    "\n",
    "df['PrevCategories'] = prev_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['BookID', 'Bookclub', 'Page', 'Course', 'Topic'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So, where have we landed: Lady or tiger? Trial or coverup? I think we just have to write something in the space below.\n",
      "Seminar to Deliberation\n",
      "Seminar Seminar, Deliberation\n",
      "Cheryl Diaz, you bring up a good point - are we truly using the information gained through the reading or our own beliefs that human nature is fundamentally selfish. You really have me thinking about this.\n",
      "Seminar to Social/Procedure/UX\n",
      "Deliberation Seminar\n",
      "I think she sends him to the door with the tiger. When the king finds out that she knew he is so pleased by her barbarism that he does nothing but inside he has now decided that she is a worthy predecessor.\n",
      "Deliberation to Seminar\n",
      "Social Seminar\n",
      "I like the \"doom\"\n",
      "Delibration to Seminar\n",
      "Deliberation Seminar\n",
      "0.9148936170212766\n"
     ]
    }
   ],
   "source": [
    "# Select all rows where 'R2Pivot' is not NaN\n",
    "non_nan_rows = df[df['R2Pivot'].notna()]\n",
    "\n",
    "# Get the indices of the non-NaN rows\n",
    "indices = non_nan_rows.index\n",
    "\n",
    "# Print the current 'R2DiscussionType' for these rows and the 'R2DiscussionType' from the previous row in the original DataFrame\n",
    "res = zip(non_nan_rows['R2DiscussionType'], df.loc[indices - 1, 'R2DiscussionType'] if indices[0] > 0 else None)\n",
    "\n",
    "counter = 0\n",
    "for idx, (next, prev) in enumerate(res):\n",
    "    if prev in non_nan_rows.iloc[idx]['R2Pivot'] and next in non_nan_rows.iloc[idx]['R2Pivot']:\n",
    "        counter += 1\n",
    "    else:\n",
    "        print(non_nan_rows.iloc[idx]['Message'])\n",
    "        print(non_nan_rows.iloc[idx]['R2Pivot'])\n",
    "        print(prev, next)\n",
    "\n",
    "print(counter/len(non_nan_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R2DiscussionType\n",
       "Seminar                  331\n",
       "Deliberation              85\n",
       "Social                    69\n",
       "UX                        47\n",
       "Procedure                 46\n",
       "Imaginative entry         17\n",
       "Other                      6\n",
       "Seminar, Deliberation      2\n",
       "Imaginative                2\n",
       "Social, Deliberation       1\n",
       "Deliberation, Seminar      1\n",
       "Social, Procedure          1\n",
       "Imaginative Entry          1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 802,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['R2DiscussionType'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LABELS == \"R2DiscussionType\":\n",
    "    df.loc[df['R2DiscussionType'] == 'Imaginative Entry', 'R2DiscussionType'] = 'Imaginative entry'\n",
    "    df.loc[df['R2DiscussionType'] == 'Imaginative', 'R2DiscussionType'] = 'Imaginative entry'\n",
    "    df.loc[df['R2DiscussionType'] == 'Deliberation, Seminar', 'R2DiscussionType'] = 'Seminar, Deliberation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "237    Yay you got it!\n",
       "Name: Message, dtype: object"
      ]
     },
     "execution_count": 804,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['R2DiscussionType'] == 'Social, Deliberation']['Message']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example seems like only Social to me. It can also be procedure due to you got it. Since it is only one sample and I do not believe it will effect our results I will just mark it as social."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LABELS == \"R2DiscussionType\":\n",
    "    df.loc[df['R2DiscussionType'] == 'Social, Deliberation', 'R2DiscussionType'] = 'Social'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "202    So, where have we landed: Lady or tiger? Trial...\n",
       "226    I agree that the princess could just banish th...\n",
       "245    Oh I see, there are two questions here. I feel...\n",
       "Name: Message, dtype: object"
      ]
     },
     "execution_count": 806,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['R2DiscussionType'] == 'Seminar, Deliberation']['Message']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full versions: \\\n",
    "1: So, where have we landed: Lady or tiger? Trial or coverup? I think we just have to write something in the space below. \\\n",
    "2: I agree that the princess could just banish the new couple away too. Do we have to agree on a single ending? \\\n",
    "3: Oh I see, there are two questions here. I feel that the princess would have led him to the tiger \n",
    "\n",
    "These are combination of two sentences in a single sample that each are from a different category. I will divide the sample into two and mark each with the corresponding category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 807,
   "metadata": {},
   "outputs": [],
   "source": [
    "if LABELS == \"R2DiscussionType\":\n",
    "    index = df.loc[df['Message'].str.startswith('Oh I see, there are two questions here')].index[0]\n",
    "    new_row = df.iloc[index].copy()\n",
    "    new_row['Message'] = \"I feel that the princess would have led him to the tiger\"\n",
    "    new_row['R2DiscussionType'] = 'Seminar'\n",
    "    new_row = pd.DataFrame(new_row).T\n",
    "    df.at[index, 'Message'] = \"Oh I see, there are two questions here.\"\n",
    "    df.at[index, 'R2DiscussionType'] = 'Deliberation'\n",
    "    df = pd.concat([df.iloc[:index + 1, ], new_row, df.iloc[index + 1:, ]], ignore_index=True) \n",
    "\n",
    "    index = df.loc[df['Message'].str.startswith('I agree that the princess could just banish the new couple away too.')].index[0]\n",
    "    new_row = df.iloc[index].copy()\n",
    "    new_row['Message'] = \"Do we have to agree on a single ending?\"\n",
    "    new_row['R2DiscussionType'] = 'Deliberation'\n",
    "    new_row = pd.DataFrame(new_row).T\n",
    "    df.at[index, 'Message'] = \"I agree that the princess could just banish the new couple away too.\"\n",
    "    df.at[index, 'R2DiscussionType'] = 'Seminar'\n",
    "    df = pd.concat([df.iloc[:index + 1, ], new_row, df.iloc[index + 1:, ]], ignore_index=True) \n",
    "\n",
    "    index = df.loc[df['Message'].str.startswith('So, where have we landed: Lady or tiger? Trial or coverup?')].index[0]\n",
    "    new_row = df.iloc[index].copy()\n",
    "    new_row['Message'] = \"I think we just have to write something in the space below.\"\n",
    "    new_row['R2DiscussionType'] = 'Deliberation'\n",
    "    new_row = pd.DataFrame(new_row).T\n",
    "    df.at[index, 'Message'] = \"So, where have we landed: Lady or tiger? Trial or coverup?\"\n",
    "    df.at[index, 'R2DiscussionType'] = 'Seminar'\n",
    "    df = pd.concat([df.iloc[:index + 1, ], new_row, df.iloc[index + 1:, ]], ignore_index=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "315    Looks good! I guess we can complete the post s...\n",
       "Name: Message, dtype: object"
      ]
     },
     "execution_count": 810,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['R2DiscussionType'] == 'Social, Procedure']['Message']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here Looks good is the social part and the rest the Procedure so I will divide again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315\n"
     ]
    }
   ],
   "source": [
    "if LABELS == \"R2DiscussionType\":\n",
    "    index = df.loc[df['Message'].str.startswith('Looks good! I guess we can complete the post survey')].index[0]\n",
    "    new_row = df.iloc[index].copy()\n",
    "    new_row['Message'] = \"I guess we can complete the post survey and submit our assignment\"\n",
    "    new_row['R2DiscussionType'] = 'Procedure'\n",
    "    new_row = pd.DataFrame(new_row).T\n",
    "    df.at[index, 'Message'] = \"Looks good!\"\n",
    "    df.at[index, 'R2DiscussionType'] = 'Social'\n",
    "    df = pd.concat([df.iloc[:index + 1, ], new_row, df.iloc[index + 1:, ]], ignore_index=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R2DiscussionType\n",
       "Seminar              334\n",
       "Deliberation          88\n",
       "Social                71\n",
       "Procedure             47\n",
       "UX                    47\n",
       "Imaginative entry     20\n",
       "Other                  6\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 812,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.value_counts('R2DiscussionType')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2columns = [col for col in df.columns if 'R2' in col]\n",
    "X = df.drop(columns=r2columns).fillna('')\n",
    "y = df[LABELS]\n",
    "\n",
    "X_train_tmp, X_test, y_train_tmp, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_tmp, y_train_tmp, test_size=0.3, random_state=42, stratify=y_train_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R2DiscussionType\n",
       "Seminar              334\n",
       "Deliberation          88\n",
       "Social                71\n",
       "Procedure             47\n",
       "UX                    47\n",
       "Imaginative entry     20\n",
       "Other                  6\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 816,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-7 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-7 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-7 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-7 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-7 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-7 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-7 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-7 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-7 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-7 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-7 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-7 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneHotEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;OneHotEncoder<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.preprocessing.OneHotEncoder.html\">?<span>Documentation for OneHotEncoder</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>OneHotEncoder()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "OneHotEncoder()"
      ]
     },
     "execution_count": 817,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = OneHotEncoder()\n",
    "enc.fit(y_train.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [],
   "source": [
    "class R2UptakeDataset(Dataset):\n",
    "    def __init__(self, X, y, tokenizer, max_length):\n",
    "        self.X = X.values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        # Fit the label binarizer and transform the labels into one-hot encoded format\n",
    "        self.labels = enc.transform(y.values.reshape(-1, 1)).toarray()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Encode the utterance using the provided tokenizer\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            self.X[idx],\n",
    "            add_special_tokens=True,\n",
    "            max_length = self.max_length,\n",
    "            return_token_type_ids=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            truncation=TRUNCATION,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        # Convert the list of strings into a one-hot encoded format\n",
    "        label = self.labels[idx]  # This should now be a binary vector instead of a list of strings\n",
    "\n",
    "        # Return the encoding and the label\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.float),\n",
    "            'token_type_ids': encoding['token_type_ids'].flatten(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 820,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels = len(df['R2Uptake'].unique())\n",
    "num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "class StratifiedSampler(Sampler):\n",
    "    def __init__(self, class_vector, batch_size):\n",
    "        self.n_splits = int(class_vector.size(0) / batch_size)\n",
    "        self.class_vector = class_vector\n",
    "\n",
    "    def gen_sample_array(self):\n",
    "        s = StratifiedShuffleSplit(n_splits=self.n_splits, test_size=0.5)\n",
    "        X = torch.randn(self.class_vector.size(0),2).numpy()\n",
    "        y = self.class_vector.numpy()\n",
    "        s.get_n_splits(X, y)\n",
    "\n",
    "        train_index, test_index = next(s.split(X, y))\n",
    "        return np.hstack([train_index, test_index])\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.gen_sample_array())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.class_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2DiscussionType\n",
      "Seminar              187\n",
      "Deliberation          49\n",
      "Social                40\n",
      "UX                    27\n",
      "Procedure             26\n",
      "Imaginative entry     11\n",
      "Other                  3\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_texts = X_train['Message'] + \"|\" + X_train['PrevCategories']\n",
    "train_labels = y_train\n",
    "\n",
    "val_texts = X_val['Message'] + \"|\" + X_val['PrevCategories']\n",
    "val_labels = y_val\n",
    "\n",
    "test_texts = X_test['Message'] + \"|\" + X_test['PrevCategories']\n",
    "test_labels = y_test\n",
    "\n",
    "longest_train_data = train_texts[train_texts.str.len().idxmax()]\n",
    "max_length = min(2 ** (len(tokenizer.tokenize(longest_train_data))-1).bit_length(), 512)\n",
    "print(train_labels.value_counts())\n",
    "\n",
    "train_data = R2UptakeDataset(train_texts, train_labels, tokenizer, max_length=max_length)\n",
    "val_data = R2UptakeDataset(val_texts, val_labels, tokenizer, max_length=max_length)\n",
    "test_data = R2UptakeDataset(test_texts, test_labels, tokenizer, max_length=max_length)\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_labels_encoded = le.fit_transform(train_labels)\n",
    "val_labels_encoded = le.transform(val_labels)\n",
    "\n",
    "#trainSampler = StratifiedSampler(torch.tensor(train_labels_encoded), BATCH_SIZE)\n",
    "#valSampler = StratifiedSampler(torch.tensor(val_labels_encoded), BATCH_SIZE)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "R2DiscussionType\n",
       "Seminar              187\n",
       "Deliberation          49\n",
       "Social                40\n",
       "UX                    27\n",
       "Procedure             26\n",
       "Imaginative entry     11\n",
       "Other                  3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 823,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I think the tiger was behind the door. Mostly because of the princess being described as \"hot-blooded and semi-barbaric.\"|The King does seem unstable enough to put his daughter on trial via the lady and the tiger method. I imagine she\\'d use her own connections to find out which door had the tiger behind it and which door had a suitor.'"
      ]
     },
     "execution_count": 824,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Deliberation'"
      ]
     },
     "execution_count": 825,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#HIDDEN_WEIGHTS = 768\n",
    "HIDDEN_WEIGHTS = 256\n",
    "DROPOUT = 0.3\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self, pretrained_model_name, num_labels):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.l1 = model_class.from_pretrained(pretrained_model_name, num_labels=self.num_labels)\n",
    "        self.pre_classifier = torch.nn.Linear(self.num_labels, HIDDEN_WEIGHTS)\n",
    "        self.dropout = torch.nn.Dropout(DROPOUT)\n",
    "        self.classifier = torch.nn.Linear(HIDDEN_WEIGHTS, self.num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        hidden_state = output_1[0]\n",
    "        pooler = hidden_state\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "        pooler = self.dropout(pooler)\n",
    "        output = self.classifier(pooler)\n",
    "        output = output.view(-1, self.num_labels)  # Reshape the output\n",
    "        return output\n",
    "\n",
    "model = BERTClass(PRETRAINED_MODEL_NAME, num_labels)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "\n",
    "weights = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    weights = weights.to('cuda')\n",
    "    \n",
    "criterion = torch.nn.CrossEntropyLoss(weight=weights)\n",
    "\n",
    "def loss_fn(outputs, targets):\n",
    "    return criterion(outputs, targets)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(model, valid_dataloader):\n",
    "    val_targets = []\n",
    "    val_outputs = []\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(valid_dataloader):\n",
    "            input_ids = batch['input_ids'].to(device, dtype=torch.long)\n",
    "            attention_mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
    "            token_type_ids = batch['token_type_ids'].to(device, dtype=torch.long)\n",
    "            labels = batch['labels'].to(device, dtype=torch.float)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            val_targets.extend(labels.cpu().detach().numpy().tolist())\n",
    "            val_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "            \n",
    "\n",
    "    val_loss /= len(valid_dataloader)\n",
    "    \n",
    "    return val_loss, val_targets, val_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ckp(checkpoint_fpath, model, enc):\n",
    "    checkpoint = torch.load(checkpoint_fpath, map_location=device)\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    enc.set_params(**checkpoint['enc'])\n",
    "    return model, enc\n",
    "\n",
    "def save_ckp(state, best_model_path):\n",
    "    torch.save(state, best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_dataloader):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        input_ids = batch['input_ids'].to(device, dtype=torch.long)\n",
    "        attention_mask = batch['attention_mask'].to(device, dtype=torch.long)\n",
    "        token_type_ids = batch['token_type_ids'].to(device, dtype=torch.long)\n",
    "        labels = batch['labels'].to(device, dtype=torch.float)\n",
    "\n",
    "        model.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= len(train_dataloader)\n",
    "    \n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(num_epochs, train_dataloader, valid_dataloader, model, optimizer, best_model_path = \"./\", patience=1):\n",
    "    valid_loss_min = np.Inf\n",
    "\n",
    "    num_not_improved = 0\n",
    "    for epoch in range(1, num_epochs):\n",
    "        print()\n",
    "        print(\"#################### Epoch {}: Training Start    ####################\".format(epoch))\n",
    "        train_loss = train(model, train_dataloader)\n",
    "        print('#################### Epoch {}: Training End      ####################'.format(epoch))\n",
    "\n",
    "        print()\n",
    "        print(\"#################### Epoch {}: Validation Start ####################\".format(epoch))\n",
    "\n",
    "        valid_loss, val_targets, val_outputs = valid(model, valid_dataloader)\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(epoch, train_loss, valid_loss))\n",
    "\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min, valid_loss))\n",
    "\n",
    "            checkpoint = {\n",
    "                        'state_dict': model.state_dict(),\n",
    "                        'enc' : enc.get_params()\n",
    "                    }\n",
    "\n",
    "            save_ckp(checkpoint, best_model_path)\n",
    "            valid_loss_min = valid_loss\n",
    "            num_not_improved = 0\n",
    "        else:\n",
    "            num_not_improved += 1\n",
    "            if num_not_improved >= patience:\n",
    "                print('Not improvement for more than:', num_not_improved)\n",
    "                break\n",
    "            \n",
    "        print(\"#################### Epoch {}: Validation End   ####################\".format(epoch))\n",
    "        print()\n",
    "\n",
    "    print(\"#################### Training finished     ####################\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No saved model found. Need to be train from scratch.\n",
      "\n",
      "#################### Epoch 1: Training Start    ####################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print('No saved model found. Need to be train from scratch.')\n",
    "trained_model = train_model(EPOCHS, train_loader, val_loader, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:45<00:00, 11.41s/it]\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_labels , test_predictions_probs = valid(trained_model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4385611116886139"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = [ prob_list == np.max(prob_list) for prob_list in test_predictions_probs ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False],\n",
       " [False, False, False, False, False, False]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1: 0.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Affirm       0.00      0.00      0.00        28\n",
      "     Clarify       0.00      0.00      0.00        10\n",
      "    Disagree       0.00      0.00      0.00         2\n",
      "   Elaborate       0.00      0.00      0.00        22\n",
      "      Filler       0.00      0.00      0.00        12\n",
      "        None       0.00      0.00      0.00        48\n",
      "\n",
      "   micro avg       0.00      0.00      0.00       122\n",
      "   macro avg       0.00      0.00      0.00       122\n",
      "weighted avg       0.00      0.00      0.00       122\n",
      " samples avg       0.00      0.00      0.00       122\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikolay/EMAI/Ljublajna/NLP/ul-fri-nlp-course-project-processingbit/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/nikolay/EMAI/Ljublajna/NLP/ul-fri-nlp-course-project-processingbit/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/nikolay/EMAI/Ljublajna/NLP/ul-fri-nlp-course-project-processingbit/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/nikolay/EMAI/Ljublajna/NLP/ul-fri-nlp-course-project-processingbit/.venv/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', accuracy_score(test_labels, test_predictions))\n",
    "print('Precision:', precision_score(test_labels, test_predictions, average='weighted'))\n",
    "print('Recall:', recall_score(test_labels, test_predictions, average='weighted'))\n",
    "print('F1:', f1_score(test_labels, test_predictions, average='weighted'))\n",
    "\n",
    "report = classification_report(test_labels, test_predictions, target_names=enc.categories_[0])\n",
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
