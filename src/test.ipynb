{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('csv', data_files='data/cleaned_data.csv')\n",
    "\n",
    "# split into train, validation and test\n",
    "train_test_dataset = dataset['train'].train_test_split(test_size=0.4)\n",
    "val_test_dataset = train_test_dataset['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "dataset['train'] = train_test_dataset['train']\n",
    "dataset['validation'] = val_test_dataset['train']\n",
    "dataset['test'] = val_test_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['course', 'book_id', 'topic', 'bookclub', 'chat_crew', 'pseudonym', 'message', 'time', 'is_answer', 'page', 'response_number', 'discussion_type', 'dialogic_spell', 'uptake', 'question', 'pivot', 'chat', 'chat_history'],\n",
       "        num_rows: 543\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['course', 'book_id', 'topic', 'bookclub', 'chat_crew', 'pseudonym', 'message', 'time', 'is_answer', 'page', 'response_number', 'discussion_type', 'dialogic_spell', 'uptake', 'question', 'pivot', 'chat', 'chat_history'],\n",
       "        num_rows: 181\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['course', 'book_id', 'topic', 'bookclub', 'chat_crew', 'pseudonym', 'message', 'time', 'is_answer', 'page', 'response_number', 'discussion_type', 'dialogic_spell', 'uptake', 'question', 'pivot', 'chat', 'chat_history'],\n",
       "        num_rows: 181\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []\n",
    "messages.append((\"system\", \"Here is the chat history of the children discussion:\"))\n",
    "messages.append((\"placeholder\", \"{history}\"))\n",
    "messages.append((\"system\", \"Classify this sentence into one class of the codebook.\"))\n",
    "messages.append((\"human\", \"{input}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"System: Here is the chat history of the children discussion:\\nSystem: Classify the sentence into one class of the codebook.\\nHuman: {'input': 'ciao', 'chat_history': 'ciao'}\""
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "prompt_template.format(input = {'input': 'ciao', 'chat_history': 'ciao'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset_csv_file = 'data/cleaned_data.csv'\n",
    "text_field = 'message'\n",
    "CLASS = 'discussion_type'\n",
    "window_size = 5\n",
    "combine_fields = ['pseudonym', 'message']\n",
    "separator = ': '\n",
    "\n",
    "data = pd.read_csv(dataset_csv_file)\n",
    "\n",
    "data[text_field] = data[combine_fields].apply(lambda x: separator.join(x.dropna().astype(str)), axis=1)\n",
    "\n",
    "history = []\n",
    "for i in range(len(data)):\n",
    "    if i >= 1 and not data.iloc[i][['book_id', 'bookclub', 'course']].equals(data.iloc[i-1][['book_id', 'bookclub', 'course']]):\n",
    "        history = []\n",
    "\n",
    "    data.at[i, 'history'] = '\\n'.join(history) if history else pd.NA\n",
    "\n",
    "    history.append(data.iloc[i][text_field])\n",
    "    if len(history) > window_size:\n",
    "        history.pop(0)\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "requests = train_data.apply(lambda x: {'input': x[text_field], 'history': [(\"human\", chat) for chat in x['history'].split('\\n')] if not pd.isna(x['history']) else []}, axis = 1)\n",
    "prompt_template = ChatPromptTemplate.from_messages(messages)\n",
    "prompts = prompt_template.batch(list(requests))\n",
    "prompts = list(map(lambda x: x.to_string(), prompts))\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': Dataset.from_dict({'text': prompts, 'label': train_data[CLASS]})\n",
    "})\n",
    "\n",
    "# train, validation and test split\n",
    "train_test_dataset = dataset_dict['train'].train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "dataset_dict['train'] = train_test_dataset['train']\n",
    "dataset_dict['validation'] = train_test_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 579\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 145\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"\n",
    "\n",
    "**Social**\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "items = ['Deliberation', 'Social']\n",
    "\n",
    "# find the item that appears first inside sentence \n",
    "def find_first(sentence, items):\n",
    "    first = len(sentence)\n",
    "    item = None\n",
    "    for i in items:\n",
    "        index = sentence.find(i)\n",
    "        if index != -1 and index < first:\n",
    "            first = index\n",
    "            item = i\n",
    "    return item\n",
    "\n",
    "find_first(sentence, items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"imdb\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 25000\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(\"./preprocessed/dataset_Discussion_with_history\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
